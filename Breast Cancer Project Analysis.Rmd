---
title: "**Breast Cancer Classification**"
author: "Dimitrios Poulimenos - 200291237"
date: "Semester 1 - 2023/24"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

```{r, include=FALSE}
# Libraries
library(mlbench)
library(dplyr)
library(ggplot2)
library(leaps)
library(glmnet)
library(bestglm)
library(corrplot)
library(plyr)
library(nclSLR)
library(tidyr)
library(combinat)
library(MASS)
```

```{r}
# Load the data
data(BreastCancer)
```

## Introduction
In this project, I will analyse the BreastCancer data set which concerns characteristics of breast tissue samples collected from 699 women in Wisconsin using fine needle aspiration cytology (FNAC). The aim of this analysis is to create multiple classifiers based on the nine cytological characteristics in order to predict the response variable which is the "Class" column in the BreastCancer data set. Through my analysis, I am going to discover useful insights into the dataset and based on my findings I will create the classifiers and discuss their results respectively.

## Data Cleaning
Before cleaning take place I will check the data shape and the structure of the data.

**Data shape:**

```{r}
# Data shape
dim(BreastCancer)
```
**Data Structure:**

```{r}
# Data structure
str(BreastCancer)
```

First things first I will check for any missing values in the data and I am going to remove the rows with the missing values (NA)

```{r}
# Find the rows with missing values
rows_with_missing_values <- which(apply(BreastCancer, 1, function(row) any(is.na(row))))

# Count the number of rows with missing values
num_missing_rows <- length(rows_with_missing_values)

# Display the result
cat("The number of rows with missing values is:", num_missing_rows, "\n")

```
As we can see there are 16 rows with NA values.
After removing the rows with the missing values lets check how our data look like.

```{r}
# Remove rows with missing values (NA)
New_BreastCancer <- na.omit(BreastCancer[, 2:11])

# Check again for rows with missing values (NA)
num_missing_rows_after_omitting <- sum(apply(New_BreastCancer, 1, function(row) any(is.na(row))))


# Display the result after omitting missing values
cat("After omitting missing values, the new number of rows with missing values is:", num_missing_rows_after_omitting, "\n")
```

## Data Preprocessing 

Now that there are no missing values in the dataset and I am ready to continue my analysis and I will proceed with the conversion of the 9 columns that I will use as predictors for my classifiers.

```{r}
# Create a vector with the predictor columns
predictor_columns <- c(1:9)

# Convert the selected columns to numeric
New_BreastCancer[, predictor_columns] = apply(New_BreastCancer[, predictor_columns], 2, function(x) as.numeric(as.character(x)))

# Check the updated data structure
str(New_BreastCancer)
```

As we can see from the data there are two classes in the "Class" column. The first one is the **benign** and the second one is the **malignant** class. Using a linear model such as logistic regression it would be better to implement three things in my data. The first thing will be the standardization of the data because it can make it easier to compare the importance of different features. The second thing is to convert the benign class to 0 and malignant class to 1 for classification purposes. And the third and the last thing is that in the new data frame the "Id" column will not be included because it represents the unique identification of each patient therefore it will have no use for our model. 

Prior to the analysis a new data frame will be created including the standardised predictor variables and the response variable which will be renamed as "y" and converted to numerical as well. 

```{r, echo=TRUE}
# Convert the response variable to numeric (0 for 'benign', 1 for 'malignant')
y <- as.numeric(New_BreastCancer[, 10]) - 1

# Extract predictor variables
X1_original <- New_BreastCancer[, -10]

# Standardize predictor variables
X1 <- scale(X1_original)

# Combine standardized predictors and response variables in a new data frame
Breast_Cancer_Final <- data.frame(X1, y)
```

```{r}
# Save the number of columns and rows for further use
p = ncol(X1)
n = nrow(X1)
```


## Exploratory Data Analysis (EDA)

Here I will conduct the exploratory data analysis to get some useful numerical and visual insights.
In the table below we can see the distribution of each class in our data.

```{r}
# Display the distribution of classes in the response variable
class_distribution <- table(Breast_Cancer_Final$y)
print(class_distribution)
```

From the table above, it's evident that the 'benign' class is the predominant class in the data with 65.01%, significantly outnumbering the 'malignant' class with 34.99%. Now I am going to investigate the relationship of each predictor variable with the the class and also the correlation between them. 

```{r, fig.width=6, fig.height=6}
# Calculate the correlation matrix
correlation <- cor(Breast_Cancer_Final[,1:10])

# Create the correlation plot
corrplot(correlation, method = 'circle', type = 'lower', insig = 'blank',
         addCoef.col = 'black',tl.col = "black", number.cex = 0.9, diag = FALSE, col = COL2('RdYlBu'))
```

---

Upon examining the correlation plot, it is evident that several predictor variables exhibit strong correlations. Notably, the highest correlation is observed between Cell.size and Cell.shape, reaching 0.91. This suggests a high degree of redundancy between these two predictors, prompting consideration for potential exclusion from our models. Furthermore, there are several instances of substantial correlations exceeding 0.7 among other predictor variables. This redundancy implies that we may not require all nine predictors for our models, and a thoughtful selection process could enhance model efficiency and interpretability. 

Additionally, investigating the correlation between the response variable (y) and predictors reveals intriguing patterns. The lowest correlation is found between Mitoses and y, registering at 0.42. In contrast, the remaining eight predictors exhibit correlations surpassing 0.69 with the response variable. Specifically, Bare.nuclei, Cell.size, and Cell.shape emerge as highly correlated with the response variable, each boasting a correlation coefficient of 0.82.

Next I am going to examine the mean of each column based on the response in order to extract some useful information.

**Benign Class:**

```{r}
# Calculate the mean of columns for the 'benign' class
benign_class_means <- round(apply(Breast_Cancer_Final[Breast_Cancer_Final[, "y"] == 0, ], 2, mean), 3)
print(benign_class_means)
```

**Malignant Class:**

```{r}
# Calculate the mean of columns for the 'malignant' class
malignant_class_means <- round(apply(Breast_Cancer_Final[Breast_Cancer_Final[, "y"] == 1, ], 2, mean), 3)
print(malignant_class_means)
```
As can be seen from the mean of the columns for each class, the values of the "malignant" class are higher than the "benign" ones. On average, tumors in the Malignant class tend to have higher values for these features compared to tumors in the Benign class.

# Classification
Having outlined the distinguishing features of each tumor class, "benign" and "malignant," the next phase involves creating five classification models to effectively differentiate between these classes. Subsequently, a thorough performance comparison will be conducted based on test errors. To ensure a fair evaluation, a consistent 10-fold validation approach will be applied across all models. The selection criterion will prioritize the model demonstrating the lowest mean squared error (MSE) on the test data, with the same set of 10 folds used for each model assessment.

## Logistic Regression using Subset Selection
Here I am, gearing up to implement subset selection, a method optimal for dimensionality reduction, aimed at identifying the most effective subset of predictor variables for the actual model. Given the scenario where the number of predictor variables is less than the number of observations (p < n), I have opted for an "exhaustive" subset selection approach over a "stepwise" one. Three distinct criteria will guide the selection process: the Bayesian Information Criterion (BIC), the Akaike Information Criterion (AIC), and the mean squared error. These criteria will serve as benchmarks for determining the most suitable subset of predictor variables for model refinement.

```{r, message=FALSE, echo=TRUE}
# Perform best subset selection using BIC for logistic regression
best_subset_BIC_model <- bestglm(Breast_Cancer_Final, family = binomial,
                                 method = "exhaustive", nvmax = p)
```

```{r}
# Retrieve the summary of the best subset selection model using BIC
bss_BIC_summary <- best_subset_BIC_model$Subsets 
```

```{r}
# # Identify model with the lowest BIC
best_BIC = which.min(bss_BIC_summary$BIC)-1

# Print the index of the model with the lowest BIC
cat("The model with the lowest BIC is at index:", best_BIC, "\n")
```
```{r, message=FALSE, echo=TRUE}
# Perform best subset selection using AIC for logistic regression
best_subset_AIC_model = bestglm(Breast_Cancer_Final, family = binomial, 
                                method="exhaustive", nvmax=p, IC = "AIC")
```

```{r}
# Retrieve the summary of the best subset selection model using AIC
bss_AIC_summary = best_subset_AIC_model$Subsets
```

```{r}
# Identify model with the lowest AIC
best_AIC = which.min(bss_AIC_summary$AIC) - 1

# Print the index of the model with the lowest AIC
cat("The model with the lowest AIC is at index:", best_AIC, "\n")
```

```{r}
# Set the seed to make the analysis reproducible
set.seed(1)

# Set the number of folds
nfolds = 10

# Sample fold-assignment index
fold_index = sample(nfolds, n, replace=TRUE)
```
 
```{r}
# K-fold validation function for subset selection, Ridge, and LASSO
reg_cv = function(X1, y, fold_ind) {
  Xy = data.frame(X1, y=y)
  nfolds = max(fold_ind)
  if(!all.equal(sort(unique(fold_ind)), 1:nfolds)) stop("Invalid fold partition.")
  cv_errors = numeric(nfolds)
  for(fold in 1:nfolds) {
    glm_fit = glm(y ~ ., data=Xy[fold_ind!=fold,], family = binomial)
    phat = predict(glm_fit, Xy[fold_ind==fold,], type = "response")
    yhat = ifelse(phat > 0.5, 1, 0) 
    yobs = y[fold_ind == fold]
    cv_errors[fold] = 1 - mean(yobs == yhat)
  }
  fold_sizes = numeric(nfolds)
  for(fold in 1:nfolds){
    fold_sizes[fold] = length(which(fold_ind==fold))
    test_error = weighted.mean(cv_errors, w=fold_sizes)
    return(test_error)
  }
}
```


```{r}
# Computes cross-validation test errors for regression models with best subset selection.
reg_bss_cv = function(X1, y, best_models, fold_index) {
  p = ncol(X1)
  test_errors = numeric(p)
  for(i in 1:p) {
    test_errors[i] = reg_cv(X1[,best_models[i,]], y, fold_index)
  }
  return(test_errors)
}

```


```{r, echo=TRUE}
# Applying the best subset selection model to assess MSE via cross-validation
bss_mse <- reg_bss_cv(X1, y, as.matrix(best_subset_AIC_model$Subsets[2:10,2:10]), fold_index)
```


```{r}
# Identify the model with the lowest cross-validation error
best_cv_model <- which.min(bss_mse)

cat("The model with the lowest error is at cross-validation index:", best_cv_model, "\n")
```

### Compare Bic, AIC and Test Error

```{r, fig.width=10}
## Create a multi-panel plot to visualize the performance metrics with optimal predictor counts:
par(mfrow = c(1, 3))

# Plot 1: BIC
plot(1:9, bss_BIC_summary$BIC[2:10], xlab="Number of predictors", ylab="BIC", type="b")
points(best_BIC, bss_BIC_summary$BIC[best_BIC + 1], col="red", pch=16)

# Plot 2: AIC
plot(1:9, bss_AIC_summary$AIC[2:10], xlab="Number of predictors", ylab="AIC", type="b")
points(best_AIC, bss_AIC_summary$AIC[best_AIC + 1], col="red", pch=16)

# Plot 3: Test error
plot(1:9, bss_mse, xlab="Number of predictors", ylab="Test error", type="b")
points(best_cv_model, bss_mse[best_cv_model], col="red", pch=16)
```

As we can see from the above plots the best model for BIC has five predictor variables and the best model for AIC has 7. However based on the test error according to 10-fold validation, the best model has only three predictor variables. Based on the test error we can observe that the model with four predictor variables has the lowe test error as the three predictor variable model and from the BIC and AIC we can tell that the models with four predictor variables are slightly different. Therefore, the model that I will choose will be the model with the four predictor variables.

Below are the **coefficients** of the selected model:

```{r}
# Create a logistic regression model with 4 predictor variables
glm_4_predictors <- glm(y ~ Cl.thickness + Cell.shape + Bare.nuclei + Bl.cromatin, data = Breast_Cancer_Final, family = binomial)

# Generate a summary of the model
summary_glm_4_predictors <- summary(glm_4_predictors)
```


```{r}
# Print the coefficients of the selected logistic regression model
coefficients_glm_4_predictors <- summary_glm_4_predictors$coefficients
print(coefficients_glm_4_predictors)
```
The Cl.thickness and Cell.shape have the higher coefficients with 1.593 and 1.729 respectively. 

And now we can print the **test error** value of the selected model:

```{r}
# Print the lowest test error
cat("Lowest error value for Subset Selection:", bss_mse[4], "\n")
```
And finally the **confusion matrix** of the selected model:

```{r}
# Transforming the X1 unseen data to a dataframe for the predict function
X1_dataframe <- as.data.frame(scale(X1_original))

# Use the fitted ridge regression model to predict probabilities
phat_bss <- predict(glm_4_predictors, newdata = X1_dataframe, type = "response")

# Convert predicted probabilities to binary predictions
yhat_bss <- ifelse(phat_bss > 0.5, 1, 0)

# Create a confusion matrix
confusion_matrix_bss <- table(Actual = y, Predicted = yhat_bss)
print(confusion_matrix_bss)
```



## Regularized Form of Logistic Regression (Ridge and LASSO)
The advantage of regularization methods is that they used in order to minimize the coefficients of the variables towards to 0 or even equal to 0. For this analysis both of Ridge and LASSO regularization methods will be used.

### Ridge Regression
In order to perform ridge regression we need to select a grid of values for the tuning parameter lambda and to fit the ridge regression model for every value of it. Primarily, we are going to select the best value of lambda based on the cross-validation with the same folds that we used for subset selection. Subsequently, log lambda against misclassification error will be plotted and the lowest error will be selected. We are going to use the plots discussed in order to see how the coefficients of the predictors behave.

```{r}
# Choose a grid of values for the tuning parameter
grid <- 10^seq(5, -3, length = 500)

# Fit a ridge regression model for each value of the tuning parameter
ridge_fit <- glmnet(X1, y, alpha = 0, standardize = FALSE, lambda = grid, family = "binomial")
```

```{r, fig.width=10}
# Choose the appropriate tuning parameter using 10-fold cross-validation error 
# with the same folds as in subset selection
ridge_cv_fit <- cv.glmnet(X1, y, alpha = 0, standardize = FALSE, lambda = grid, nfolds = nfolds, foldid = fold_index,
                          family = "binomial", type.measure = "class")

# Create a 1x2 layout for plots
par(mfrow = c(1, 2))

# Plot ridge path
plot(ridge_fit, xvar = "lambda", col = 1:10, label = TRUE)

# Examine the effect of the tuning parameter on the MSE
plot(ridge_cv_fit)
```

The left plot clearly illustrates that the coefficients of the predictor variables progressively shrink towards zero as lambda approaches 5. Conversely, the second plot indicates that the minimum lambda value falls between the two dotted vertical lines. Consequently, identifying the precise minimum lambda value for cross-validation is essential.

```{r}
# Identify the optimal value for the tuning parameter in ridge regression
lambda_ridge_min <- ridge_cv_fit$lambda.min

# Print the optimal lambda value
cat("The optimal value for the tuning parameter (lambda) in Ridge regression is:", lambda_ridge_min, "\n")
```
Below are the **coefficients** of Ridge Regression model:

```{r}
# Identify the index of the optimal lambda in the ridge regression model
which_lambda_ridge <- which(ridge_cv_fit$lambda == lambda_ridge_min)

# Print the rounded parameter estimates for the optimal value of the tuning parameter in ridge regression
rounded_ridge_parameter_estimates <- round(coef(ridge_fit, s = lambda_ridge_min), 3)
print(rounded_ridge_parameter_estimates)
```
Now check the coefficients of a normal glm fit:

```{r,echo=TRUE}
# Fit a logistic regression model using all predictor variables
glm_fit <- glm(y ~ ., data = Breast_Cancer_Final, family = binomial)
```

```{r}
# Round the coefficients of the logistic regression model
rounded_glm_fit_coefficients <- round(glm_fit$coefficients, 3)
print(rounded_glm_fit_coefficients)
```

The observed contraction of coefficients towards zero indicates the successful functioning of our model. Nevertheless, it's noteworthy that certain coefficients, such as those associated with "Epith.c.size" and "Mitoses," have experienced an increase instead.

And now we can print the **test error** value of Ridge Regression:

```{r}
# Obtain the corresponding cross-validation error for the final ridge regression model
ridge_mse <- ridge_cv_fit$cvm[which_lambda_ridge]

# Print the lowest test error
cat("Lowest error value for Ridge Regression:", ridge_mse, "\n")
```

And finally the **confusion matrix** of Ridge Regression:

```{r}
# Use the fitted ridge regression model to predict probabilities
phat_ridge <- predict(ridge_fit, s = lambda_ridge_min, newx = X1, type = "response")

# Convert predicted probabilities to binary predictions
yhat_ridge <- ifelse(phat_ridge > 0.5, 1, 0)

# Create a confusion matrix
confusion_matrix_ridge <- table(Actual = y, Predicted = yhat_ridge)
print(confusion_matrix_ridge)
```

### LASSO Regression
The Lasso (Least Absolute Shrinkage and Selection Operator) is a regularization technique similar to ridge regression, but with a key difference. In addition to the sum of squared coefficients penalty term, the Lasso adds a penalty term proportional to the absolute values of the coefficients. We are going to use the same methodology as with the Ridge Regression.

```{r}
# Choose grid of values for the tuning parameter
grid1 = 10^seq(5, -3, length=500)

# Fit a LASSO regression for each value of the tuning parameter 
lasso_fit = glmnet(X1, y, alpha=1, standardize=FALSE, lambda=grid1, family = "binomial")
```

```{r, fig.width=10}
# Examine the effect of the tuning parameter on the parameter estimates 
par(mfrow=c(1,2))
plot(lasso_fit, xvar="lambda", col=1:10, label=TRUE)

# Compute 10-fold cross-validation error using the same folds as in ss and ridge regression
lasso_cv_fit = cv.glmnet(X1, y, alpha=1, standardize=FALSE, lambda=grid, nfolds=nfolds, foldid=fold_index,
                         family = "binomial", type.measure = "class")
plot(lasso_cv_fit)
```

The left plot reveals a distinctive pattern where certain predictor coefficients drop out earlier than others. Notably, "Mitoses" is the first to drop out, followed by "Epith.c.size," "Marg.adhesion," "Normal.nucleoli," "Bl.cromatin," and "Cl.thickness." The last trio of predictor coefficients, namely "Cell.shape," "Cell.size," and "Bare.nuclei", appear to drop out simultaneously. As observed in the right plot,  the minimum lambda value lies between the two dotted vertical lines.

```{r}
# Identify the optimal value for the tuning parameter
lambda_lasso_min = lasso_cv_fit$lambda.min

# Print the optimal lambda value
cat("The optimal value for the tuning parameter (lambda) in LASSO regression is:", lambda_lasso_min, "\n")
```

Below are the **coefficients** of LASSO Regression:

```{r}
# Print the rounded parameter estimates for the optimal value of the tuning parameter in lasso regression
rounded_lasso_parameter_estimates <- round(coef(lasso_fit, s = lambda_lasso_min), 3)
print(rounded_lasso_parameter_estimates)
```

And now we can print the **test error** value of LASSO Regression:

```{r}
# Corresponding cross validation error
which_lambda_lasso = which(lasso_cv_fit$lambda == lambda_lasso_min)
lasso_final_mse = lasso_cv_fit$cvm[which_lambda_lasso]

# Print the lowest test error
cat("Lowest error value for LASSO Regression:", lasso_final_mse, "\n")
```

Finally the **confusion matrix** of LASSO Regression:

```{r}
# Use the fitted lasso regression model to predict probabilities
phat_lasso <- predict(lasso_fit, s = lambda_lasso_min, newx = X1, type = "response")

# Convert predicted probabilities to binary predictions
yhat_lasso <- ifelse(phat_lasso > 0.5, 1, 0)

# Create a confusion matrix
confusion_matrix_lasso <- table(Actual = y, Predicted = yhat_lasso)
print(confusion_matrix_lasso)
```

## Discriminant Analyis Method
Discriminant analysis serves as a powerful tool for classification, particularly in the context of this project, where the objective is to identify the optimal classifier for the variable "Class." In linear discriminant analysis (LDA), the key assumption is that the classes have a common covariance matrix, meaning that the variance of each predictor variable is the same across all classes. On the other hand, quadratic discriminant analysis (QDA) relaxes the assumption of a common covariance matrix, allowing each class to have its own covariance matrix. This increased flexibility enables QDA to capture more intricate relationships between predictor variables within each class, accommodating scenarios where the variability within classes differs significantly.

### Linear Discriminant Analysis 
The objective here is to perform feature selection by exploring all possible subsets of predictor variables. The subset that exhibits the lowest test error through cross-validation will be chosen as the optimal set of features.

```{r, echo=TRUE}
# Create a vector that contain the names of all the predictor variables 
colnames_vector <- colnames(Breast_Cancer_Final)[1:9]

# Now compute all the possible subsets of the variables 
subset_combinations = unlist(lapply(1:length(colnames_vector),  combinat::combn,
                                    x = colnames_vector, simplify = FALSE),
                                    recursive = FALSE)
```

```{r}
# Cross validation function for LDA 
reg_cv_lda = function(x1, y, fold_ind){
  Xy = data.frame(x1, y=y)
  nfolds = max(fold_ind)
  if(!all.equal(sort(unique(fold_ind)), 1:nfolds)) stop("Invalid fold partition.")
  cv_errors = numeric(nfolds)
  for (fold in 1:nfolds) {
    tmp_fit = lda(y~., data = Xy[fold_ind!=fold,])
    phat = predict(tmp_fit, Xy[fold_ind == fold,])
    yhat = phat$class
    yobs = y[fold_ind==fold]
    cv_errors[fold] = 1 - mean(yobs == yhat)
  }
  fold_sizes = numeric(nfolds)
  for (fold in 1:nfolds) fold_sizes[fold] = length(which(fold_ind==fold))
  test_error_lda = weighted.mean(cv_errors, w=fold_sizes)
}
```

```{r}
# Compute cross-validation test errors for LDA models with various predictor subsets
LDA_errors <- unlist(lapply(subset_combinations, function(subset) {
  selected_predictors <- as.data.frame(Breast_Cancer_Final[, unlist(subset)])
  reg_cv_lda(selected_predictors, y, fold_index)
}))
```


```{r}
best_lda_subset_index <- which.min(LDA_errors)

# Print the index of the subset with the lowest cross-validation error for LDA
cat("The subset with the lowest cross-validation error for LDA is at index:", best_lda_subset_index, "\n")
```

Now that we know the index of the subset with the lowest test error based on cross validation it is time to extract the predictor variables that this subset uses to achieve that.

```{r}
# Print the predictor variables included in the best LDA subset
best_lda_subset <- subset_combinations[best_lda_subset_index]
print(best_lda_subset)
```

By fitting the lda model we can now see the **group means** of the model:

```{r, echo=TRUE}
# Perform LDA
lda_fit <- lda(y ~ Cl.thickness + Cell.size + Epith.c.size + Bare.nuclei + Bl.cromatin + 
                 Mitoses, data = Breast_Cancer_Final)
```


```{r}
# Print the summary of LDA
lda_fit
```

As it can be seen from the group means of the above summary the "benign" class have negative means compared to the "malignant" ones which has positive means. This indicates that the tumors labeled as malignant are more likely to have higher values in the variables "Cl.thickness", "Cell.size", "Epith.c.size", "Bare.nuclei", "Bl.cromatin", and "Mitoses".

Next we print the **test error** value of the LDA model:

```{r}
# Print the cross-validation error for the best LDA subset
best_lda_error <- LDA_errors[best_lda_subset_index]

cat("The cross-validation error for the best LDA subset is:", best_lda_error, "\n")
```

Finally below is the **confusion matrix** of LDA:

```{r}
X1_dataframe <- as.data.frame(scale(X1_original))

# Use the fitted LDA model to predict probabilities
phat_lda <- predict(lda_fit, newdata = X1_dataframe, type="response")

# Extract the predicted class probabilities from the posterior component
phat_values <- phat_lda$posterior[, 2]  

# Convert predicted probabilities to binary predictions
yhat_lda <- ifelse(phat_values > 0.5, 1, 0)

# Create a confusion matrix
confusion_matrix_lda <- table(Actual = y, Predicted = yhat_lda)
print(confusion_matrix_lda)
```

### Quadratic Discriminant Analysis

```{r}
# Cross validation function for QDA
reg_cv_qda = function(x1, y, fold_ind){
  Xy = data.frame(x1, y=y)
  nfolds = max(fold_ind)
  if(!all.equal(sort(unique(fold_ind)), 1:nfolds)) stop("Invalid fold partition.")
  cv_errors = numeric(nfolds)
  for (fold in 1:nfolds) {
    tmp_fit = qda(y~., data = Xy[fold_ind!=fold,])
    phat = predict(tmp_fit, Xy[fold_ind == fold,])
    yhat = phat$class
    yobs = y[fold_ind==fold]
    cv_errors[fold] = 1 - mean(yobs == yhat)
  }
  fold_sizes = numeric(nfolds)
  for (fold in 1:nfolds) fold_sizes[fold] = length(which(fold_ind==fold))
  test_error_lda = weighted.mean(cv_errors, w=fold_sizes)
}
```

```{r}
# Compute cross-validation test errors for QDA models with various predictor subsets
QDA_errors <- unlist(lapply(subset_combinations, function(subset) {
  selected_predictors <- as.data.frame(Breast_Cancer_Final[, unlist(subset)])
  reg_cv_qda(selected_predictors, y, fold_index)
}))
```


```{r}
best_qda_subset_index <- which.min(QDA_errors)

# Print the index of the subset with the lowest cross-validation error for LDA
cat("The subset with the lowest cross-validation error for LDA is at index:", best_qda_subset_index, "\n")
```

Just like we did for LDA now that we know the index of the subset with the lowest test error based on cross validation it is time to extract the predictor variables that this subset uses to achieve that.

```{r}
# Print the predictor variables included in the best LDA subset
best_qda_subset <- subset_combinations[best_qda_subset_index]
print(best_qda_subset)
```
By fitting the QDA model we can now see the **group means** of the model:

```{r, echo=TRUE}
# Perform QDA 
qda_fit = qda(y~ Cl.thickness+ Marg.adhesion + Epith.c.size  + Bare.nuclei, 
               data = Breast_Cancer_Final)
```

```{r}
# Print the summary of QDA
qda_fit
```

Once again just like in the LDA model the mean of the selected variables for "benign" class are all negative and for "malignant" class are positive. Again, this indicates that the cancer tumors labeled as benign are more likely to have smaller values in the variables "Cl.thickness", "Marg.adhesion", "Epith.c.size", and "Bare.nuclei".

Next we print the **test error** value of the QDA model:

```{r}
# Print the cross-validation error for the best QDA subset
best_qda_error <- QDA_errors[best_qda_subset_index]

cat("The cross-validation error for the best QDA subset is:", best_qda_error, "\n")
```

Finally below is the **confusion matrix** of QDA:

```{r}
X1_dataframe <- as.data.frame(scale(X1_original))

# Use the fitted LDA model to predict probabilities
phat_qda <- predict(qda_fit, newdata = X1_dataframe, type="response")

# Extract the predicted class probabilities from the posterior component
phat_values_qda <- phat_qda$posterior[, 2]  

# Convert predicted probabilities to binary predictions
yhat_qda <- ifelse(phat_values_qda > 0.5, 1, 0)

# Create a confusion matrix
confusion_matrix_qda <- table(Actual = y, Predicted = yhat_qda)
print(confusion_matrix_qda)
```


## Models Comparisson
Having all the above models is now time to compare all of them in order to check which one is the best model based on the lowest test error provided by implementing cross validation.

In the table below are all the test errors from all the implemented models:

```{r}
# Create a matrix with 4 columns and 1 row
error_values = matrix(c(round(bss_mse[4], 4), round(ridge_mse, 4), round(lasso_final_mse, 4), round(best_lda_error, 4), round(best_qda_error, 4)), ncol=5, byrow=TRUE)
 
# Specify the column names and row names of the matrix
colnames(error_values) = c('BSS-4','Ridge','LASSO','LDA', 'QDA')
rownames(error_values) <- c('Test Error')
 
# Convert the matrix to a data frame
final=as.table(error_values)
 
# Print the data frame
print(final, justify = "right")
```

The cross-validation results indicate notable differences in test errors among the models. The highest observed test error originates from the best subset selection, registering at 0.0580, whereas the lowest is associated with the LASSO model, showcasing a notably lower value of 0.0307. It's noteworthy that both Ridge and LASSO exhibit comparable test errors, demonstrating close performance metrics. Similarly, the LDA and QDA models align closely, revealing test errors of 0.0366 and 0.0351, respectively. These findings underscore the nuanced distinctions in predictive accuracy across the implemented models.

## Comclusion
After a comprehensive analysis of the various metrics and comparisons among the models, my definitive recommendation is to adopt the Ridge regression model. This decision is grounded in its standout performance, boasting the lowest test error value derived from cross-validation. Notably, the Ridge model incorporates all nine cytological characteristics, suggesting that considering each of these variables is integral to achieving the model's minimal test error.

This preference for the Ridge regression model underscores the significance of every characteristic in the classification task. In contrast, alternative models utilizing fewer variables exhibit higher test errors, reinforcing the crucial role played by each cytological feature in discerning between "benign" and "malignant" tumor classes. The Ridge regression model emerges as the optimal choice, epitomizing the importance of a comprehensive approach in achieving superior predictive accuracy.